{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "from itertools import combinations, chain\n",
    "from  more_itertools import unique_everseen\n",
    "import time\n",
    "import random\n",
    "#import data and preprocess\n",
    "data = open(\"adult.csv\",\"r\")\n",
    "\n",
    "#sampling improvement for Apriori\n",
    "samplingfactor = .6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data and each observation as a list within a data list structure\n",
    "#add _i to data value indicating i'th variable\n",
    "def clean(dta,samplingfactor):\n",
    "    datalist = []\n",
    "    for line in dta:\n",
    "        linesep = line.split(\", \")\n",
    "        for attribute in linesep:\n",
    "            newattribute = attribute  + \"_\" + str(linesep.index(attribute))\n",
    "            linesep[linesep.index(attribute)] = newattribute\n",
    "        datalist.append(linesep)\n",
    "        \n",
    "    random.shuffle(datalist) #randomizes data order\n",
    "    datalist = [datalist[i] for i in range(0,int(round(samplingfactor*len(datalist))))] #return the first sampfactor\n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining C1 - candidate set with one attribute\n",
    "def gen_C1(dta):\n",
    "    count_dict = {}\n",
    "    for observation in dta:\n",
    "        for val in observation:\n",
    "            if val not in count_dict:\n",
    "                count_dict[val] = 1\n",
    "            else:\n",
    "                count_dict[val] += 1\n",
    "    \n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining L1 - frequent set derived by C1\n",
    "# Q: what is sampling factor referring to?\n",
    "def gen_L1(dic, min_supp, samplingfactor):\n",
    "    L1 = []\n",
    "    for var in dic:\n",
    "        if float(dic[var])/float(n_obs)>min_supp*samplingfactor:\n",
    "            L1.append(var)\n",
    "    return L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if k-1 subsets of k itemset are all in L_k-1\n",
    "# if any k-1 subset of itemset is not frequent, then rule the k-itemset out\n",
    "def has_infreq_subset(k,L,c):\n",
    "    for i in list(combinations(c,k-1)):\n",
    "        if i not in L:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate C2 from L1\n",
    "def gen_C2(k,L):\n",
    "    Ck = list(combinations(L,k))\n",
    "    for c in Ck:\n",
    "        if has_infreq_subset(k,L,c):\n",
    "            Ck.remove(c) # if it has infrequent subset, remove the candidate\n",
    "        else:\n",
    "            pass\n",
    "    return Ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_Lk(Ck,min_supp,dta,samplingfactor):\n",
    "    count_dict = {}\n",
    "    for c in Ck:\n",
    "        count_dict[c] = 0\n",
    "        \n",
    "    for observation in dta:\n",
    "        for c in Ck:\n",
    "            if set(c).issubset(observation):\n",
    "                count_dict[c] +=1\n",
    "    \n",
    "    Lk = []\n",
    "    for c in Ck:\n",
    "        if float(count_dict[c])/n_obs>min_supp*samplingfactor:\n",
    "            Lk.append(c)\n",
    "            \n",
    "    return Lk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_Ck(k,L):\n",
    "    flatten = [item for subtuple in L for item in subtuple] # flatten all candidates\n",
    "    uniqueflatten = list(unique_everseen(flatten)) # remove all duplicate candidates\n",
    "    Ck = list(combinations(uniqueflatten,k)) # all possible combinations of k length\n",
    "    for c in Ck:\n",
    "        if has_infreq_subset(k,L,c):\n",
    "            Ck.remove(c)\n",
    "        else:\n",
    "            pass\n",
    "    return Ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Apriori(k,L,dta,min_supp,samplingfactor):\n",
    "    l_of_L = [] # store different Lk\n",
    "    while L!=[]:\n",
    "        if k==2:\n",
    "            Ck = gen_C2(k,L)\n",
    "        else:\n",
    "            Ck = gen_Ck(k,L)\n",
    "        L = gen_Lk(Ck,min_supp,cleandata,samplingfactor)\n",
    "        l_of_L.append(L)\n",
    "        k+=1\n",
    "        \n",
    "    return l_of_L[0:len(l_of_L)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first few lines of input data are:\n",
      "\n",
      "[['26_0', 'Private_1', '35917_2', 'HS-grad_3', '9_4', 'Married-civ-spouse_5', 'Sales_6', 'Husband_7', 'White_8', 'Male_9', '0_10', '0_11', '40_12', 'United-States_13', '<=50K\\n_14'], ['19_0', '?_1', '307837_2', 'Some-college_3', '10_4', 'Never-married_5', '?_6', 'Own-child_7', 'White_8', 'Male_9', '0_10', '0_11', '40_12', 'United-States_13', '<=50K\\n_14'], ['26_0', 'Private_1', '101027_2', 'HS-grad_3', '9_4', 'Married-civ-spouse_5', 'Sales_6', 'Husband_7', 'White_8', 'Male_9', '0_10', '0_11', '48_12', 'United-States_13', '<=50K\\n_14']]\n",
      "The frequent itemsets are:\n",
      "\n",
      "('Private_1', 'White_8')\n",
      "('Private_1', 'Male_9')\n",
      "('Private_1', '0_10')\n",
      "('Private_1', '0_11')\n",
      "('Private_1', 'United-States_13')\n",
      "('Private_1', '<=50K\\n_14')\n",
      "('Married-civ-spouse_5', 'White_8')\n",
      "('Married-civ-spouse_5', 'Male_9')\n",
      "('Married-civ-spouse_5', '0_10')\n",
      "('Married-civ-spouse_5', '0_11')\n",
      "('Married-civ-spouse_5', 'United-States_13')\n",
      "('White_8', 'Male_9')\n",
      "('White_8', '0_10')\n",
      "('White_8', '0_11')\n",
      "('White_8', '40_12')\n",
      "('White_8', 'United-States_13')\n",
      "('White_8', '<=50K\\n_14')\n",
      "('Male_9', '0_10')\n",
      "('Male_9', '0_11')\n",
      "('Male_9', 'United-States_13')\n",
      "('Male_9', '<=50K\\n_14')\n",
      "('0_10', '0_11')\n",
      "('0_10', '40_12')\n",
      "('0_10', 'United-States_13')\n",
      "('0_10', '<=50K\\n_14')\n",
      "('0_11', '40_12')\n",
      "('0_11', 'United-States_13')\n",
      "('0_11', '<=50K\\n_14')\n",
      "('40_12', 'United-States_13')\n",
      "('40_12', '<=50K\\n_14')\n",
      "('United-States_13', '<=50K\\n_14')\n",
      "('Private_1', 'White_8', '0_10')\n",
      "('Private_1', 'White_8', 'United-States_13')\n",
      "('Private_1', 'Male_9', '0_10')\n",
      "('Private_1', 'Male_9', 'United-States_13')\n",
      "('Private_1', '0_10', '0_11')\n",
      "('Private_1', '0_10', '<=50K\\n_14')\n",
      "('Private_1', '0_11', '<=50K\\n_14')\n",
      "('White_8', 'Male_9', '0_11')\n",
      "('White_8', 'Male_9', '<=50K\\n_14')\n",
      "('White_8', '0_10', 'United-States_13')\n",
      "('White_8', '0_10', 'Married-civ-spouse_5')\n",
      "('White_8', '0_11', 'United-States_13')\n",
      "('White_8', '0_11', 'Married-civ-spouse_5')\n",
      "('White_8', 'United-States_13', '<=50K\\n_14')\n",
      "('Male_9', '0_10', 'United-States_13')\n",
      "('Male_9', '0_10', 'Married-civ-spouse_5')\n",
      "('Male_9', '0_11', 'United-States_13')\n",
      "('Male_9', '0_11', 'Married-civ-spouse_5')\n",
      "('Male_9', 'United-States_13', '<=50K\\n_14')\n",
      "('0_10', '0_11', '<=50K\\n_14')\n",
      "('0_10', '0_11', '40_12')\n",
      "('0_10', 'United-States_13', 'Married-civ-spouse_5')\n",
      "('0_11', 'United-States_13', 'Married-civ-spouse_5')\n",
      "('Private_1', 'White_8', '0_10', 'Male_9')\n",
      "('Private_1', 'White_8', '0_10', '<=50K\\n_14')\n",
      "('Private_1', 'White_8', 'United-States_13', '0_11')\n",
      "('Private_1', 'White_8', 'Male_9', '0_11')\n",
      "('Private_1', 'White_8', '0_11', '<=50K\\n_14')\n",
      "('Private_1', '0_10', 'United-States_13', 'Male_9')\n",
      "('Private_1', '0_10', 'United-States_13', '0_11')\n",
      "('Private_1', '0_10', 'United-States_13', '<=50K\\n_14')\n",
      "('Private_1', '0_10', 'Male_9', '0_11')\n",
      "('Private_1', 'United-States_13', 'Male_9', '0_11')\n",
      "('Private_1', 'United-States_13', '0_11', '<=50K\\n_14')\n",
      "('White_8', '0_10', 'United-States_13', '0_11')\n",
      "('White_8', '0_10', 'Male_9', '0_11')\n",
      "('White_8', '0_10', 'Male_9', '<=50K\\n_14')\n",
      "('White_8', '0_10', '0_11', '<=50K\\n_14')\n",
      "('White_8', 'United-States_13', 'Male_9', '0_11')\n",
      "('White_8', 'United-States_13', 'Male_9', '<=50K\\n_14')\n",
      "('White_8', 'United-States_13', '0_11', '<=50K\\n_14')\n",
      "('0_10', 'United-States_13', 'Male_9', '0_11')\n",
      "('0_10', 'United-States_13', 'Male_9', '<=50K\\n_14')\n",
      "('0_10', 'United-States_13', '0_11', '<=50K\\n_14')\n",
      "('0_10', 'Male_9', '0_11', '<=50K\\n_14')\n",
      "('United-States_13', 'Male_9', '0_11', '<=50K\\n_14')\n",
      "('Private_1', 'White_8', '0_10', '<=50K\\n_14', 'United-States_13')\n",
      "('Private_1', 'White_8', '0_10', 'United-States_13', '0_11')\n",
      "('Private_1', 'White_8', '<=50K\\n_14', 'United-States_13', '0_11')\n",
      "('Private_1', '0_10', '<=50K\\n_14', 'United-States_13', '0_11')\n",
      "('White_8', '0_10', 'Male_9', '<=50K\\n_14', '0_11')\n",
      "('White_8', '0_10', 'Male_9', 'United-States_13', '0_11')\n",
      "('White_8', '0_10', '<=50K\\n_14', 'United-States_13', '0_11')\n",
      "('0_10', 'Male_9', '<=50K\\n_14', 'United-States_13', '0_11')\n",
      "\n",
      "\n",
      "The runtime for this algorithm (with a sampling factor=0.6 and min_supp=0.6) is 2.4127070903778076\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "cleandata = clean(data,samplingfactor)\n",
    "n_obs = len(cleandata)\n",
    "\n",
    "print('The first few lines of input data are:'+'\\n')\n",
    "print(cleandata[0:3])\n",
    "\n",
    "C1 = gen_C1(cleandata)\n",
    "L1 = gen_L1(C1,0.75,samplingfactor)\n",
    "freqsets = Apriori(2,L1,cleandata,0.6,samplingfactor)\n",
    "\n",
    "print('The frequent itemsets are:'+'\\n')\n",
    "for i in freqsets:\n",
    "    for j in i:\n",
    "        print(j)\n",
    "\n",
    "time2 = time.time()\n",
    "\n",
    "testtime = time2-time1\n",
    "print('\\n')\n",
    "print('The runtime for this algorithm (with a sampling factor=0.6 and min_supp=0.6) is {}'.format(testtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
